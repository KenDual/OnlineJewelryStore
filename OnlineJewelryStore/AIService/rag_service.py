"""
RAG Service - Core Pipeline
K·∫øt h·ª£p vector search v·ªõi Llama LLM ƒë·ªÉ t·∫°o AI advisor
"""

import requests
import json
from typing import List, Dict, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGService:
    def __init__(
        self, 
        embeddings_manager,
        ollama_url: str = "http://localhost:11434",
        model_name: str = "llama3.1:8b"
    ):
        """
        Initialize RAG Service
        
        Args:
            embeddings_manager: EmbeddingsManager instance v·ªõi loaded index
            ollama_url: Ollama API endpoint
            model_name: Llama model name
        """
        self.em = embeddings_manager
        self.ollama_url = ollama_url
        self.model_name = model_name
        self.api_endpoint = f"{ollama_url}/api/generate"
    
    def search_products(
        self,
        query: str,
        category: Optional[str] = None,
        min_price: Optional[float] = None,
        max_price: Optional[float] = None,
        top_k: int = 5
    ) -> List[Tuple[Dict, float]]:
        """
        Search products v·ªõi filters
        
        Args:
            query: User query
            category: T√™n category (optional)
            min_price: Gi√° t·ªëi thi·ªÉu (optional)
            max_price: Gi√° t·ªëi ƒëa (optional)
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            
        Returns:
            List of (product, score) tuples
        """
        # Vector search
        results = self.em.search(query, top_k=top_k * 2)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter
        
        # Apply filters
        filtered_results = []
        for product, score in results:
            # Filter by category
            if category and product.get('CategoryName', '').lower() != category.lower():
                continue
            
            # Filter by price
            product_price = product.get('MinPrice', product.get('BasePrice', 0))
            
            if min_price and product_price < min_price:
                continue
            
            if max_price and product_price > max_price:
                continue
            
            filtered_results.append((product, score))
            
            if len(filtered_results) >= top_k:
                break
        
        logger.info(f"Search returned {len(filtered_results)} products after filtering")
        return filtered_results
    
    def generate_context(self, products: List[Tuple[Dict, float]]) -> str:
        """
        T·∫°o context string t·ª´ search results
        
        Args:
            products: List of (product, score) tuples
            
        Returns:
            Formatted context string
        """
        if not products:
            return "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong database."
        
        context_parts = []
        context_parts.append("=== S·∫¢N PH·∫®M C√ì S·∫¥N ===\n")
        
        for idx, (product, score) in enumerate(products, 1):
            product_info = [
                f"[S·∫£n ph·∫©m {idx}]",
                f"- T√™n: {product['ProductName']}",
                f"- ID: {product['ProductID']}",
                f"- Danh m·ª•c: {product.get('CategoryName', 'N/A')}",
            ]
            
            # Gi√°
            min_price = product.get('MinPrice', product.get('BasePrice', 0))
            max_price = product.get('MaxPrice', product.get('BasePrice', 0))
            
            if min_price == max_price:
                product_info.append(f"- Gi√°: {min_price:,.0f} VND")
            else:
                product_info.append(f"- Gi√°: {min_price:,.0f} - {max_price:,.0f} VND")
            
            # Description (truncate n·∫øu qu√° d√†i)
            desc = product.get('Description', '')
            if desc:
                if len(desc) > 150:
                    desc = desc[:150] + "..."
                product_info.append(f"- M√¥ t·∫£: {desc}")
            
            # Metals
            if product.get('AvailableMetals'):
                product_info.append(f"- Ch·∫•t li·ªáu: {product['AvailableMetals']}")
            
            # Stock
            stock = product.get('TotalStock', 0)
            stock_status = "C√≤n h√†ng" if stock > 0 else "H·∫øt h√†ng"
            product_info.append(f"- Tr·∫°ng th√°i: {stock_status}")
            
            # Reviews
            if product.get('ReviewCount', 0) > 0:
                rating = product.get('AvgRating', 0)
                count = product.get('ReviewCount', 0)
                product_info.append(f"- ƒê√°nh gi√°: {rating:.1f}‚≠ê ({count} reviews)")
            
            # Relevance score
            product_info.append(f"- ƒê·ªô ph√π h·ª£p: {score:.2f}")
            
            context_parts.append("\n".join(product_info))
            context_parts.append("")  # Empty line
        
        return "\n".join(context_parts)
    
    def create_prompt(
        self, 
        user_query: str, 
        context: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        T·∫°o prompt cho Llama model
        
        Args:
            user_query: User's question
            context: Product context from RAG
            conversation_history: Optional chat history
            
        Returns:
            Formatted prompt
        """
        system_prompt = """B·∫°n l√† t∆∞ v·∫•n vi√™n chuy√™n nghi·ªáp c·ªßa c·ª≠a h√†ng trang s·ª©c tr·ª±c tuy·∫øn.

NHI·ªÜM V·ª§:
- T∆∞ v·∫•n s·∫£n ph·∫©m d·ª±a TR√äN d·ªØ li·ªáu c√≥ s·∫µn
- G·ª£i √Ω 1-3 s·∫£n ph·∫©m PH√ô H·ª¢P NH·∫§T v·ªõi nhu c·∫ßu kh√°ch h√†ng
- Gi·∫£i th√≠ch T·∫†I SAO s·∫£n ph·∫©m ph√π h·ª£p
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† chuy√™n nghi·ªáp

GI·ªöI H·∫†N:
- CH·ªà t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m c√≥ trong danh s√°ch ƒë∆∞·ª£c cung c·∫•p
- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin s·∫£n ph·∫©m kh√¥ng c√≥ trong database
- KH√îNG t∆∞ v·∫•n v·ªÅ ƒë·∫∑t h√†ng, thanh to√°n, v·∫≠n chuy·ªÉn
- N·∫øu kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p, l·ªãch s·ª± t·ª´ ch·ªëi v√† g·ª£i √Ω t√¨m ki·∫øm kh√°c

FORMAT TR·∫¢ L·ªúI:
1. Ch√†o h·ªèi ng·∫Øn g·ªçn
2. Ph√¢n t√≠ch nhu c·∫ßu c·ªßa kh√°ch
3. G·ª£i √Ω 1-3 s·∫£n ph·∫©m v·ªõi: T√™n, Gi√°, L√Ω do ph√π h·ª£p, ID s·∫£n ph·∫©m
4. K·∫øt th√∫c v·ªõi c√¢u h·ªèi m·ªü (n·∫øu c·∫ßn l√†m r√µ th√™m)

V√ç D·ª§ OUTPUT:
"D·∫°, em xin ch√†o anh/ch·ªã!

Anh/ch·ªã ƒëang t√¨m nh·∫´n c∆∞·ªõi v√†ng trong kho·∫£ng gi√° 10-20 tri·ªáu. Em xin gi·ªõi thi·ªáu 2 s·∫£n ph·∫©m ph√π h·ª£p:

üîπ **Nh·∫´n C∆∞·ªõi V√†ng 18K Classic** (ID: 123)
   - Gi√°: 15,500,000 VND
   - Thi·∫øt k·∫ø ƒë∆°n gi·∫£n, thanh l·ªãch, ph√π h·ª£p ng√†y c∆∞·ªõi
   - V√†ng 18K b·ªÅn ƒë·∫πp, kh√¥ng b·ªã phai m√†u
   - ƒêang c√≤n h√†ng, ƒë∆∞·ª£c kh√°ch h√†ng ƒë√°nh gi√° 4.8‚≠ê

üîπ **Nh·∫´n C∆∞·ªõi V√†ng Tr·∫Øng Sang Tr·ªçng** (ID: 145)
   - Gi√°: 18,200,000 VND  
   - Ki·ªÉu d√°ng hi·ªán ƒë·∫°i, sang tr·ªçng
   - V√†ng tr·∫Øng 18K cao c·∫•p
   - C√≤n h√†ng

Anh/ch·ªã th√≠ch ki·ªÉu thi·∫øt k·∫ø ƒë∆°n gi·∫£n hay c√≥ ƒëi·ªÉm nh·∫•n ƒë√° qu√Ω ·∫°?"
"""
        
        prompt_parts = [
            system_prompt,
            "\n=== D·ªÆ LI·ªÜU S·∫¢N PH·∫®M ===",
            context,
            "\n=== C√ÇU H·ªéI C·ª¶A KH√ÅCH H√ÄNG ===",
            f"User: {user_query}",
            "\n=== TR·∫¢ L·ªúI C·ª¶A T∆Ø V·∫§N VI√äN ===",
            "Assistant:"
        ]
        
        # Th√™m conversation history n·∫øu c√≥
        if conversation_history:
            history_text = "\n=== L·ªäCH S·ª¨ TR√í CHUY·ªÜN ===\n"
            for msg in conversation_history[-3:]:  # Ch·ªâ l·∫•y 3 tin nh·∫Øn g·∫ßn nh·∫•t
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                history_text += f"{role}: {content}\n"
            
            # Insert history before current query
            prompt_parts.insert(-3, history_text)
        
        return "\n".join(prompt_parts)
    
    def call_llama(
        self, 
        prompt: str, 
        max_tokens: int = 800,
        temperature: float = 0.7
    ) -> Optional[str]:
        """
        Call Ollama API ƒë·ªÉ generate response
        
        Args:
            prompt: Full prompt string
            max_tokens: Max response length
            temperature: Creativity level (0-1)
            
        Returns:
            Generated response text ho·∫∑c None n·∫øu l·ªói
        """
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40
                }
            }
            
            logger.info(f"Calling Ollama API: {self.api_endpoint}")
            
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=60  # 60 gi√¢y timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                logger.info(f"‚úÖ Llama response generated ({len(generated_text)} chars)")
                return generated_text
            else:
                logger.error(f"‚ùå Ollama API error: {response.status_code}")
                logger.error(response.text)
                return None
                
        except requests.exceptions.Timeout:
            logger.error("‚ùå Ollama API timeout")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error calling Ollama: {e}")
            return None
    
    def chat(
        self,
        user_query: str,
        category: Optional[str] = None,
        min_price: Optional[float] = None,
        max_price: Optional[float] = None,
        conversation_history: Optional[List[Dict]] = None,
        top_k: int = 3
    ) -> Dict:
        """
        Main chat function - RAG pipeline
        
        Args:
            user_query: User question
            category, min_price, max_price: Optional filters
            conversation_history: Chat history
            top_k: Number of products to retrieve
            
        Returns:
            Dictionary v·ªõi response v√† metadata
        """
        logger.info(f"Processing query: {user_query}")
        
        # 1. Search relevant products
        products = self.search_products(
            query=user_query,
            category=category,
            min_price=min_price,
            max_price=max_price,
            top_k=top_k
        )
        
        # 2. Generate context
        context = self.generate_context(products)
        
        # 3. Create prompt
        prompt = self.create_prompt(user_query, context, conversation_history)
        
        # 4. Call Llama
        response = self.call_llama(prompt)
        
        if not response:
            return {
                "success": False,
                "message": "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "products": []
            }
        
        # 5. Extract product IDs from response (optional)
        product_ids = [p[0]['ProductID'] for p in products[:3]]
        
        return {
            "success": True,
            "message": response,
            "products": [
                {
                    "id": p[0]['ProductID'],
                    "name": p[0]['ProductName'],
                    "price": p[0].get('MinPrice', p[0].get('BasePrice', 0)),
                    "category": p[0].get('CategoryName', ''),
                    "image": p[0].get('MainImageURL', ''),
                    "score": p[1]
                }
                for p in products[:3]
            ]
        }


# ===== USAGE EXAMPLE =====
if __name__ == "__main__":
    from embeddings_manager import EmbeddingsManager
    
    # 1. Load embeddings manager
    em = EmbeddingsManager()
    em.load_model()
    em.load_index("data/faiss_index")
    
    # 2. Initialize RAG service
    rag = RAGService(em)
    
    # 3. Test chat
    result = rag.chat(
        user_query="T√¥i mu·ªën t√¨m nh·∫´n c∆∞·ªõi v√†ng gi√° kho·∫£ng 15 tri·ªáu",
        min_price=10000000,
        max_price=20000000
    )
    
    print("\n=== RAG Response ===")
    print(result['message'])
    
    print("\n=== Suggested Products ===")
    for p in result['products']:
        print(f"- {p['name']} ({p['price']:,.0f} VND)")